{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c2ed1f-0df8-4f1f-aee1-12d0812aab9c",
   "metadata": {},
   "source": [
    "# Laís Notebook 13: Redes Neurais Convolucionais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e841da-a64f-46bc-8392-1b4af4520c42",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee804f31-9d25-4693-acba-65ea0b9d3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8c62782-6cb7-48ca-9b8f-fdbef8ada302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, plot_confusion_matrix\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Add,AveragePooling2D,Dense, AvgPool2D,BatchNormalization, ReLU, DepthwiseConv2D, Reshape, Permute,Conv2D, MaxPool2D, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a34bf6a-a6ba-4179-bbd7-ad30f1fdf883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ec03f0-4649-4fea-b64a-2c2237983331",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 02:37:18.928607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 02:37:18.929028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 02:37:18.962329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 02:37:18.962726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 02:37:18.963133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-02-16 02:37:18.963521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(False)\n",
    "devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "tf.config.experimental.set_memory_growth(devices[1], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ec96d-14ec-4e0f-913f-c02f7878712b",
   "metadata": {},
   "source": [
    "### Diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65bb977-1390-4681-bd4f-67d11dea4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_IM = \"./images_split/\"\n",
    "PATH_NN = \"./neural_networks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b08296-ce00-40e3-80c7-762eacd93729",
   "metadata": {},
   "source": [
    "### Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05e2ef-f47d-4c23-9d68-e56caa037603",
   "metadata": {},
   "source": [
    "- Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aecdb444-0797-4827-bd6e-9a9be5f7e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (128, 128)\n",
    "NUM_CLASSES = 5\n",
    "INPUT_SHAPE = (128, 128, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f022cb3-7c8a-4e04-bc5a-84ad6ed71a60",
   "metadata": {},
   "source": [
    "- Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e432ba7b-12ca-4118-8de5-e1b82ce3992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "patience = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b1e31-da96-4fc4-9689-1e92e8ca7377",
   "metadata": {},
   "source": [
    "### Funções de avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8ab269-a5f6-4bb8-a29f-c85a318762d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, training_time, total_params, name_model, path, i):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    metrics = {\n",
    "        \"Total Params\": total_params, \n",
    "        \"Training Time\": training_time, \n",
    "        \"Test Accuracy\": accuracy, \n",
    "        \"Test F1 Weightet\": f1, \n",
    "        \"Test Precision Weighted\": precision, \n",
    "        \"Test Recall Weighted\": recall}\n",
    "    \n",
    "    df_new = pd.DataFrame(data=metrics, index=[\"Experimento \" + str(i)])\n",
    "    \n",
    "    if os.path.exists(path + \"Metrics_\" + name_model + \".csv\"):\n",
    "        df = pd.read_csv(path + \"Metrics_\" + name_model + \".csv\", index_col=0)\n",
    "        df.append(df_new).to_csv(path + \"Metrics_\" + name_model + \".csv\", header=True, index=True)\n",
    "    else:\n",
    "        df_new.to_csv(path + \"Metrics_\" + name_model + \".csv\", header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f8cfc1-b3af-4744-a1bd-1706a017222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_scorer(model, name_model, class_names, test_data, test_labels, path, i):\n",
    "    class estimator:\n",
    "        _estimator_type = ''\n",
    "        classes_= []\n",
    "\n",
    "        def __init__(self, model, classes):\n",
    "            self.model = model\n",
    "            self._estimator_type = 'classifier'\n",
    "            self.classes_ = classes\n",
    "\n",
    "        def predict(self, X):\n",
    "            y_prob= self.model.predict(X)\n",
    "            y_pred = y_prob.argmax(axis=1)\n",
    "            return y_pred\n",
    "\n",
    "\n",
    "    classifier = estimator(model, class_names)\n",
    "    \n",
    "    cm = plot_confusion_matrix(estimator=classifier, X=test_data, y_true=test_labels, xticks_rotation=45, cmap='Greys')\n",
    "    cm.ax_.set_title('Matriz de Confusão - ' + name_model + 'InceptionV3 Exp' + str(i))\n",
    "    cm.ax_.set_xlabel(\"Classes previstas\")\n",
    "    cm.ax_.set_xticklabels(class_names)\n",
    "    cm.ax_.set_ylabel(\"Classes reais\")\n",
    "    cm.ax_.set_yticklabels(class_names)\n",
    "    plt.savefig(path + \"CM\" + str(i) + '_' + name_model + \".jpg\", dpi=115, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1f20c-c616-4ee4-9508-62cf71170d6f",
   "metadata": {},
   "source": [
    "### Criando a ShuffleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ddd3d76-b799-4085-9fae-504673062e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_shuffle(x, groups):  \n",
    "    _, width, height, channels = x.get_shape().as_list()\n",
    "    group_ch = channels // groups\n",
    "\n",
    "    x = Reshape([width, height, group_ch, groups])(x)\n",
    "    x = Permute([1, 2, 4, 3])(x)\n",
    "    x = Reshape([width, height, channels])(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d167d1d7-46bb-411b-8158-48948fa4a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_unit(x, groups, channels,strides):\n",
    "    y = x\n",
    "    x = Conv2D(channels//4, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "\n",
    "    x = channel_shuffle(x, groups)\n",
    "    \n",
    "    x = DepthwiseConv2D(kernel_size = (3,3), strides = strides, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides == (2,2):\n",
    "        channels = channels - y.shape[-1]\n",
    "        \n",
    "    x = Conv2D(channels, kernel_size = 1, strides = (1,1),padding = 'same', groups=groups)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if strides ==(1,1):\n",
    "        x =Add()([x,y])\n",
    "        \n",
    "    if strides == (2,2):\n",
    "        y = AvgPool2D((3,3), strides = (2,2), padding = 'same')(y)\n",
    "        x = concatenate([x,y])\n",
    "    \n",
    "    x = ReLU()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21af7161-a99e-446d-b9ef-395cfbf2d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shuffle_Net(n_classes, start_channels, input_shape = (224,224,3)):\n",
    "    groups = 2\n",
    "    input = Input(input_shape)\n",
    "\n",
    "    x =  Conv2D (24,kernel_size=3,strides = (2,2), padding = 'same', use_bias = True)(input)\n",
    "    x =  BatchNormalization()(x)\n",
    "    x =  ReLU()(x)\n",
    "    \n",
    "    x = MaxPool2D (pool_size=(3,3), strides = 2, padding='same')(x)\n",
    "\n",
    "    repetitions = [3,7,3]\n",
    "\n",
    "    for i,repetition in enumerate(repetitions):\n",
    "        channels = start_channels * (2**i)\n",
    "\n",
    "        x  = shuffle_unit(x, groups, channels,strides = (2,2))\n",
    "\n",
    "        for i in range(repetition):\n",
    "            x = shuffle_unit(x, groups, channels,strides=(1,1))\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    output = Dense(n_classes,activation='softmax')(x)\n",
    "\n",
    "    model = Model(input, output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddadb976-4e87-4627-a44e-c63fe49774ef",
   "metadata": {},
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d08a8-0674-4708-b57a-6cee22be9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    path = \"exp\" + str(i)\n",
    "    train_dir = PATH_IM + path + \"/train/\"\n",
    "    validation_dir = PATH_IM + path + \"/val/\"\n",
    "    test_dir = PATH_IM + path + \"/test/\"\n",
    "    \n",
    "    # Generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle = False)\n",
    "    \n",
    "    # Verificando se a pasta para os logs do modelo existe\n",
    "    model = \"ShuffleNet/\"\n",
    "    LOGS_PATH = PATH_NN + model\n",
    "\n",
    "    if not os.path.isdir(LOGS_PATH):\n",
    "        os.makedirs(LOGS_PATH)\n",
    "    \n",
    "    if not os.path.isdir(LOGS_PATH + \"Logs/\"):\n",
    "        os.makedirs(LOGS_PATH + \"Logs/\")\n",
    "    \n",
    "    if not os.path.isdir(LOGS_PATH + \"Checkpoints/\"):\n",
    "        os.makedirs(LOGS_PATH + \"Checkpoints/\")\n",
    "    \n",
    "    # Callbacks\n",
    "    csv_logger = callbacks.CSVLogger(LOGS_PATH + \"Logs/\" + \"training\" + str(i) + \".log\")\n",
    "    early_stopping = callbacks.EarlyStopping(monitor=\"val_acc\", patience=patience)\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(\n",
    "        filepath=LOGS_PATH + \"Checkpoints/\" + \"checkpoint\" + str(i), \n",
    "        monitor=\"val_acc\", \n",
    "        save_best_only=True, \n",
    "        save_weights_only=True, \n",
    "        mode=\"max\")\n",
    "    \n",
    "    # ShuffleNet com ajustes nas camadas de saída para igualar ao Modelo Chollet\n",
    "    shufflenet = Shuffle_Net(NUM_CLASSES, 200, INPUT_SHAPE)\n",
    "\n",
    "    last = shufflenet.layers[-2]\n",
    "    x = layers.Flatten()(last.output)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    predictions = layers.Dense(NUM_CLASSES, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    # Criando e compilando o modelo\n",
    "    model = models.Model(shufflenet.input, predictions)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(learning_rate=1e-4), metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    \n",
    "    # Treinando o modelo e verificando o tempo de treino\n",
    "    print(\"Modelo sendo treinado\")\n",
    "    beginning = time.time()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_generator, \n",
    "        steps_per_epoch=100, \n",
    "        epochs=EPOCHS, \n",
    "        validation_data=validation_generator, \n",
    "        validation_steps=50, \n",
    "        callbacks=[csv_logger, early_stopping, model_checkpoint],\n",
    "        verbose=0)\n",
    "\n",
    "    end = time.time()\n",
    "    training_time = end - beginning\n",
    "    training_time = time.ctime(training_time)\n",
    "    training_time = training_time[11:-5]\n",
    "    \n",
    "    # Salvando o melhor modelo\n",
    "    model.load_weights(LOGS_PATH + \"Checkpoints/\" + \"checkpoint\" + str(i))\n",
    "    model.save(LOGS_PATH + \"ShuffleNet_exp\" + str(i) + \".hdf5\")\n",
    "    \n",
    "    # Progresso do modelo\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'b', label='Treino')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validação', color=\"orange\")\n",
    "    plt.title('Acurácia de Treino e Validação')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "    plt.savefig(LOGS_PATH + \"Accuracy\" + str(i) +\".jpg\", dpi=115, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b', label='Treino')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validação', color=\"orange\")\n",
    "    plt.title('Perda do Treino e Validação')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda')\n",
    "    plt.legend()\n",
    "    plt.savefig(LOGS_PATH + \"Loss\" + str(i) + \".jpg\", dpi=115, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Testando o modelo\n",
    "    STEP_SIZE_TEST = test_generator.n // test_generator.batch_size\n",
    "    test_generator.reset()\n",
    "\n",
    "    y_pred = model.predict(test_generator, steps=STEP_SIZE_TEST, verbose=1)\n",
    "    \n",
    "    # Avaliando o modelo\n",
    "    y_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "    labels = (train_generator.class_indices)\n",
    "    labels = dict((v, k) for k, v in labels.items())\n",
    "\n",
    "    y_pred = [labels[k] for k in y_pred]\n",
    "    \n",
    "    y_true = test_generator.filenames\n",
    "    y_true = [i[:i.find('/')] for i in y_true]\n",
    "    \n",
    "    evaluate_model(y_true, y_pred, training_time, \"1,375,525\", \"ShuffleNet\", LOGS_PATH, i)\n",
    "    \n",
    "    # Plotando a matriz de confusão\n",
    "    class_names = [\"Cercospirose\", \"Saudável\", \"Ferrugem\", \"Bicho Mineiro\", \"Phoma\"]\n",
    "\n",
    "    true_labels = []\n",
    "\n",
    "    for filename in y_true:\n",
    "        for k, v in labels.items():\n",
    "            if filename == v:\n",
    "                true_labels.append(k)\n",
    "    \n",
    "    confusion_matrix_scorer(model, \"ShuffleNet\", class_names, test_generator, true_labels, LOGS_PATH, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
